{"cells":[{"cell_type":"markdown","source":["Mount a Databricks Azure blob (using read-only access and secret key pair), access one of the files in the blob as a DBFS path, then unmount the blob.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> The mount point **must** start with `/mnt/`."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n### Creating a Shared Access Signature (SAS) URL\nAzure provides you with a secure way to create and share access keys for your Azure Blob Store without compromising your account keys.\n\nMore details are provided <a href=\"http://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1\" target=\"_blank\"> in this document</a>.\n\nThis allows access to your Azure Blob Store data directly from Databricks distributed file system (DBFS).\n\nAs shown in the screen shot, in the Azure Portal, go to the storage account containing the blob to be mounted. Then:\n\n1. Select Shared access signature from the menu.\n2. Click the Generate SAS button.\n3. Copy the entire Blog service SAS URL to the clipboard.\n4. Use the URL in the mount operation, as shown below.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/DataFrames-MSFT/create-sas-keys.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; margin-top: 20px; padding: 10px\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Create the mount point with `dbutils.fs.mount(source = .., mountPoint = .., extraConfigs = ..)`.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If the directory is already mounted, you receive the following error:\n\n> Directory already mounted: /mnt/temp-training\n\nIn this case, use a different mount point such as `temp-training-2`, and ensure you update all three references below."],"metadata":{}},{"cell_type":"code","source":["SasURL = \"https://dbtraineastus2.blob.core.windows.net/?sv=2017-07-29&ss=b&srt=sco&sp=rl&se=2023-04-19T06:32:30Z&st=2018-04-18T22:32:30Z&spr=https&sig=BB%2FQzc0XHAH%2FarDQhKcpu49feb7llv3ZjnfViuI9IWo%3D\"\nindQuestionMark = SasURL.index('?')\nSasKey = SasURL[indQuestionMark:len(SasURL)]\nStorageAccount = \"dbtraineastus2\"\nContainerName = \"training\"\nMountPoint = \"/mnt/temp-training\"\n\ndbutils.fs.mount(\n  source = \"wasbs://%s@%s.blob.core.windows.net/\" % (ContainerName, StorageAccount),\n  mount_point = MountPoint,\n  extra_configs = {\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (ContainerName, StorageAccount) : \"%s\" % SasKey}\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["List the contents of a subdirectory in directory you just mounted:"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/temp-training"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%fs head /mnt/temp-training/auto-mpg.csv"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Real world Example: \n  World Wide Importers daily data"],"metadata":{}},{"cell_type":"markdown","source":["### Copying the sample data\n Sample data is located at https://aka.ms/wwi_sample_data. \n >Copy this link in browser to get the SAS link and then copy the container to your own storage account."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.mount(\n  source = \"wasbs://<CONTAINER NAME>@<STORAGE ACCOUNT NAME>.blob.core.windows.net\",\n  mount_point = \"/mnt/wwwi_raw\",\n  extra_configs = {\"fs.azure.account.key.<STORAGE ACCOUNT NAME>.blob.core.windows.net\":\"STORAGE ACCOUNT KEY\"})"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%fs ls '/mnt/wwwi_raw/orders/stg_ext'"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["orderDF = spark.read.parquet(\"/mnt/wwwi_raw/orders/stg_ext/orders_2018-12-11.parquet\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["orderDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["orderDF.createOrReplaceTempView(\"orders_temp\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM  orders_temp \"))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql\nSELECT * FROM  orders_temp where customerID = 905 order by orderdate desc"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["bikeSharingDayDF = (spark\n  .read                                                # Call the read method returning a DataFrame\n  .option(\"inferSchema\",\"true\")                        # Option to tell Spark to infer the schema\n  .option(\"header\",\"true\")                             # Option telling Spark that the file has a header\n  .csv(\"/mnt/training/bikeSharing/data-001/day.csv\"))  # Option telling Spark where the file is"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["bikeSharingDayDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["bikeSharingDayDF.count()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["bikeSharingDayDF.show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["orderLinesDF = spark.read.parquet(\"/mnt/wwwi_raw/OrderLines/stg_ext/orderlines_2018-12-11.parquet\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["orderLinesDF.createOrReplaceTempView('orderlines')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%sql\nselect * from orderlines order by PickingCompletedWhen desc limit 10;"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Databases and tables\nAzure Databricks allows us to create Databases and Tables similar to an RDBMS. Databases and tables can be *\"managed\"* or *\"unmanaged\"*. **Managed(Internal)** and **Unmanaged(External)** tables and databases are only different in two ways:\n  1. Managed tables are located in the ADB's default Blob storage account and is only accessible through ADB. But Unmanaged tables can store their data in any storage account (mounted or direct access).\n  2. Dropping an Unmanaged(\"External\") table WILL NOT affect the data but dropping a managed table WILL DELETE the underlying data."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE DATABASE wwi_adb_training\n-- LOCATION '/mnt/wwwi_raw/wwi_adb_training_db/' -- specifying location will make your table an \"external\" table or \"UN-managed\""],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS wwi_adb_training.ORDERLINES_PERM_EXTERNAL_TABLE;\nCREATE TABLE wwi_adb_training.ORDERLINES_PERM_EXTERNAL_TABLE \n--USING CSV  -- Default is Parquet \n--PARTITIONED BY (date) -- we can partition the table using any of the columns\n--LOCATION '/mnt/wwwi_raw/test_table/' -- specifying location will make your table an \"external\" table or \"UN-managed\"\nas SELECT * FROM orderlines;"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\nshow create table wwi_adb_training.ORDERLINES_PERM_EXTERNAL_TABLE"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Since the underlying data of a managed Table can be changed directly in order for the metadata to be refreshed we need to run the \"REFRESH\" command."],"metadata":{}},{"cell_type":"code","source":["%sql\nselect count(*) from wwi_adb_training.ORDERLINES_PERM_EXTERNAL_TABLE ; "],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["ELT using External tables: \nOnce we have an external table created we can use Azure Data Factory to load files in to the uderlying Blob location for our ELT Notebook to use for further processing."],"metadata":{}},{"cell_type":"code","source":["%sql\nrefresh table wwi_adb_training.ORDERLINES_PERM_EXTERNAL_TABLE;"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%sql\nselect count(*) from wwi_adb_training.ORDERLINES_PERM_EXTERNAL_TABLE ; "],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["from pyspark.sql.functions import * ## Importing all SQL functions to use in our Notebook"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%sql\nselect count(*) as numberOfLineItems, orderid from wwi_adb_training.ORDERLINES_PERM_EXTERNAL_TABLE where quantity > 50 group by orderid having count(*) >= 2 order by 2;"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Lab - Building an ELT pipeline\n  1. Create a new external databases for staging and DW layers of the WWI's new Data Warehouse in Azure Databricks.\n  2. Create tables respective tables for Orders, Customers, Orderlines in each database without hard coding the column names and types.\n >*Hint 1: Can a view make it easier?*\n \n >*Hint 2: Remember the schema of a DW table differs with DB tables*\n  3. Load your staging tables by placing the source system files in staging directory\n  4. Write a simple transform, load script (*Hint: you can store your logic in permanent views*)\n>Note: Blob Storage is an imutable storage and as a result Azure Databricks would not normally support *UPDATE* or *UPSERT* statements. **Azure Databricks Delta** could be the answer!"],"metadata":{}}],"metadata":{"name":"Training","notebookId":2106013403448263},"nbformat":4,"nbformat_minor":0}
